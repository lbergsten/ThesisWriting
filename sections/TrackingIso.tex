Our measurement of $H\rightarrow WW^*\rightarrow \ell\nu\ell\nu$ events relies on the accurate determination of a number of physics objects withing the detector: jets, missing transverse energy ($E_T^{miss}$), electrons, and muons. The final state particles of our interaction consist of one electron, one muon, and two neutrinos, which appear as missing transverse energy in the detector since they cannot be directly detected. Because our search focuses on VBF production Higgs, we also require two jets in the final state. Each type of physics object is studied by a particular performance group which is tasked with providing recomendations for reconstruction, identification, isolation and measurements of efficiency, scale, and resolultion. Biasses or omissions in any of these would severely impact the precision of our analysis and so understanding the uncertainties associated with each reconstructed object is critical. In this chapter I will briefly outline the algorithms used for tracking and isolation and determining their relevant uncertainties, for jets, missing transverse energy, electrons, and muons. Particular attention will be paid to muons as I spent a significant amount of time working on applying corrections to muon momentum scale and resolution to better model measured muon data. 
\section{Jets}
As previously mentioned, quarks and gluons emitted from high energy hard scattering do not appear in the detector directly. As quarks and gluons reachehigh enough energy low energy gluons are radiated until partons are able to bind into color-neutral hadrons and these hadrons are seen collimated in groups in the detector as "jets". Their energy and momentum are used in physics analyses as proxies for the intial scattered partons. Jet calibration seeks to determine jet energy scale and jet energy and angular resolution as accurately as possible. Pile-up presents the main difficulty in jet calibration - multiple interactions occuring in the detector at once creates significant, often hadronic, background. The hard interaction of interest must be separated from pile-up background (which is most often soft). During Run-1, the ATLAS experiment reconstructed jets using either only the calorimeter or the tracker, though most often the calorimeter. Topological clusters of calorimeter cells (topo-clusters) were used to trace jet tracks. At the end of Run-1, the jet energy scale (JES) correction factor used to calibrate jets to the particle level was re-calculated using additional track information from the Inner Detector and Muon Spectrometer, which greatly improved jet resolution \cite{JetRun1}. 

Beginning in Run-2, a new algorithm for jet reconstruction was developed which took advantage of the improvements shown by including information from the tracker. 'Particle flow' utilizes the higher momentum for low-energy charged particles and the greater angular resolution of single charged particles from the tracker. This is complemented by the calorimeter's ability to reconstruct both charged and neutral particles and the calorimeters higher energy resolution at high energy. The calorimeter also has an extended acceptance so in the forward region only calorimeter topo-clusters are used. One potential difficulty with a 'particle flow' algorithm is that double counting particles is possible if each tracker reconstructed jet isn't correctly matched to its calorimeter signal. This is avoided in the algorithm through the condition that if a particle's track measurement is used, its corresponding energy must be subtracted from the calorimeter measurement. The success of the algorithm in removing only energy deposits from the tracked jet represents a key criteria for its overall performance \cite{ParticleFlow}.

\begin{figure}[!h]
        \centering
    \includegraphics[width=.85\textwidth]{Pictures/ParticleFlow.png}
    \caption{Flow chart of the particle flow algorithm beginning with track selection and ending with charged particles, and changed/unchanged topo-clusers \cite{ParticleFlow}.}
    \label{fig:ParticleFlow}
\end{figure}

Topo-clusters are described by two main properties - $\epsilon$, which represents for each particle the fraction of true energy deposited in the cluster out of the total true energy deposited in all topo-clusters and $\rho$, the purity or fraction ofa particle's true energy which lies within the topo-cluster. High $\rho$, high $\epsilon$ topo-clusters separate out contributions from different particles and so are easier to apply hadronic shower subtraction \cite{ParticleFlow}.

Jet tracks need to meet strict criteria to be considered, these include at least nine higts in silicon detectors, no missing Pixel hits, $\eta$<.5, and $40>p_T>0.5$GeV. These are refered to as a "tight" selection. Tracks which obey these criteria and do not deposit enough energy to create topo-clusters are still included. High $p_T$ tracks are excluded because their poor isolation. In addition, tracks which are identified as electrons or muons are excluded \cite{ParticleFlow}. 

With topo-clusters and tracks assembled, the algorithm next must match each to one another. Topo-clusters are ranked through the distance metric 
\begin{equation}
\Delta R' = \sqrt{(\frac{\Delta \phi}{\sigma_\phi})^2+(\frac{\Delta \eta}{\sigma_\eta})^2}
\end{equation}
where $\sigma_\phi$ and $\sigma_\eta$ denote angular topo-cluster withs. A requirement that $E^{clus}/p^{trk}>0.1$ is applied so that the energy of the topo-cluster must contain more a significant portion of the energy of the track. This requirement gives rejects about $30-40\%$ of the incorrect topo-clusters at $p_T>5 GeV $ (this is less pronounced at lower $p_T$ and very rarely rejects the correct cluster. Distance selection is made such that closes topo-cluster to each track in $\Delta R'$ is taken to be the correct match. This is very successful at $p_T>5$GeV. If no topo-cluster is within a cone of $\Delta R'= 1.64$, it is assumed the particle did not form a topo-cluster in the calorimeter \cite{ParticleFlow}. 

Topo-clusters are thus matched to particle tracks and next the energy the particle deposits mucst be substracted from the calorimeter. The average energy deposited by a particle with momentum $p^{trk}$ is given $<E_{dep}>=p^{trk}<E_{ref}^{clus}/p_{ref}^{trk}>$ where $<E_{ref}^{clus}/p_{ref}^{trk}>$ is calculated using single-particle samples without pile-up by summing topo-cluster energies within $\Delta R = 0.4$ about the track position. These are calculated at varying $p_T$ and $\eta$ values to capture effects from detector geometry and shower development. Particles often split their energy between multiple topo-clusters and this can be determined through the significance of the difference between expected energy and that of the matched topocluster. If significance is below -1, a split shower recovery procedure runs: topo-clusters within a cone of $\Delta R =0.2$ about the track position are considered to be matched to the track. This new full set of matched clusters is considered for energy subtraction \cite{ParticleFlow}. 

Energy subtraction is performed cell-by-cell (unless $<E_{dep}>$ is greater than the energy of the total matched topo-clusters, in which case they are all removed). Rings are formed in $\eta,\phi$ about the extrapolated track and are one calorimeter cell wide. The average energy density in each ring is computed and the ring with the highes energy density is subtracted first. This continues to lower density rings until $<E_{dep}>$ is reached. Finally, after cell-by-cell subtraction, remnant energy clusters are removed if they're considered purely from shower fluctuations and otherwise remain. Ideally now the set of selected tracks and remaining topo-clusters together represent the reconstructed event without double counting between the subdetectors. An example display of particle flow events is shown in Figure \ref{fig:ParticleFlowExample}

\begin{figure}[!h]
        \centering
    \includegraphics[width=.4\textwidth]{Pictures/ParticleFlowExample.png}
    \caption{Sum of transverse momenta of neutral and charged particle flow objects in an area $\Delta \eta \times \Delta\phi = 0.2 \times 0.2$ from a 2017 event with mean number of interactions per beam crossing $\mu=38$ \cite{JETEtmiss}}
    \label{fig:ParticleFlowExample}
\end{figure}

Jet finding algorithms aim to approximate the hadron generators of calorimeter and track-based jets in the detector. Fragmentation and hadronization which occur before jets are visible in the detector and jet finding algorithms bridge the divide between observable jet objects and theoretical predictions in terms of QCD. There are several jet algorithms in use currently, and key properties of all are collinear and infrared safety. This means that neither splitting a jet collinearly nor soft emissions should change jet structure and is important because without these qualities perturbation theory diverges at high orders. In addition, modelling hadronization, underlying event, and pile-up is inprecise, so the ideal theory would be minimally sensitive to these affects. Anti $k_t$ is the jet algorithm used by ATLAS and is infrared collinar-safe by construction and soft-resilient. Distance between pseudojets $i$ and $j$, $d_{ij}$ and the distance between pseudojet $i$ and the beam $d_{iB}$ are defined. The algorithm loops through these distances beginning with the smallest distance. If $d_{ij}$ is smallest, pseudojets $i$ and $j$ are combined, if $d_{iB}$ is smallest then $i$ is a jet and removed from the list. This continues iteratively until all jets are defined. The distances are defined:
\begin{equation}
d_{ij} = min(k_{ti}^{2p},k_{tj}^{2p})\frac{\Delta_{ij}^2}{R^2} , \\
d_{iB} = k_{ti}^{2p}
\end{equation}
where $\Delta_{ij}^2 = (y_i-y_j)^2+(\phi_i-\phi_j)^2$ and $k_{ti}$ is the transverse momentum of particle i. $p$ is a new paarameter which signifies the relative power of energy versus geometrical scales ($\Delta_{ij}$). If $p=1$ the algorithm follows one know as $k_t$, if $p=0$ the algorithm is Cambridge/Aachen, and if $p=-1$ the anti-$k_t$ algorithm results. One notable characteristic of this algorithm is its resilience to soft radiation while maintaining collinear safety, which is why it's the prevailing jet algorithm used on the ATLAS experiment \cite{antikt}. 

Jet reconstruction and isolation doesn't end with particle flow and the anti-$k_t$ algorithm. Further corrections have to be applied to model data with Monte Carlo simulations. MC simulation are calibrated to better model pile-up and to improve jet angular resolution. Global sequential calibrations are next calculated with MC using calorimeter, track, and muon chamber information to improve jet resolution. Following this, data is used to constrain uncertainties with a combination of different measurements with different known samples. Finally, these calibrations lead to final measurements of jet energy scale (JES) and jet energy resolution (JER) and provide reconstructed jet events in MC and data as well as a number recommended uncertainties for physics analyses to use.  Figures \ref{fig:ScaleRes} show jet energy scale and resolution as a function of $p_T$ for PFlow jets modeling with the anti-$k_t$ algorithm after JES corrections are applied. Fully combined systematic uncertaines are also shown for each \cite{jetscaleres}.

\begin{figure}[!h]
    \centering
  \begin{minipage}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{Pictures/ParticleFlowScale.png}
  \end{minipage}
  \hspace{.5cm}
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Pictures/ParticleFlowResolution.png}
  \end{minipage}
    \caption{ a.) data-to-simulation ratio for average jet $p_T$ as a function of jet $p_T$. Three techniques shown as well as their their combination (black) and the combination total uncertainty. PFlow Anti-$k_t$ jets with R=0.4 and JES correction. b.) relative jet energy resolution as a function of $p_T$ for anti-$k_t$ PFlow jets with $R=0.4$ in 2017. JES calibrations applied and compared to MC with full systematic uncertainties \cite{JETEtmiss}.}
    \label{fig:ScaleRes}
\end{figure}

\section{Missing transverse energy}
Collider experiments like ATLAS begin with momentum solely in the plane of the beam, as two protons collide into one another. Conservation of momentum implies that in the plane transverse to the beam ($x-y$) the sum of momentum from all interaction by-products is zero. This is quite often not the case and the non-zero transverse momentum from any interaction is termed $E_T^{miss}$. Missing transverse energy is a sign of final state neutrinos, which are massless and neutrally charged and so go undetected within ATLAS. $E_T^{miss}$ could also point to new particles that cannot be directly detected like neutralinos or dark matter particles. Finally, $E_T^{miss}$ could signify interacting particles which evade detection in ATLAS due to detector acceptance or poor reconstruction \cite{METSig}. 

Missing transverse energy is determined using a combination of all reconstructed particles in an event. This is challenging because it involves all detector components and final particle types. There are multiple algorithms for this summation which yield $E_T^{miss}$ variables and our $H\rightarrow WW$ measurement utilizes multiple of them. This section will discuss $E_T^{miss}$ reconstruction and performance followed by definitions of a few other key $E_T^{miss}$ observables. 
Reconstructed $E_T^{miss}$ calculations take into account both \textit{hard} and \textit{soft} event signals. Hard-events are composed of fully reconstructed and calibrated particles like electrons, muons, photons, $\tau$-leptons, and jets. Soft events are reconstructed charged-particle tracks from the hard-scatter vertext, all objects except those considered \textit{hard}. Reconstruction for all particle types happen independently, which means that the same signal may be used to identify two distinct particles. This double-counting is taken into account in $E_T^{miss}$ resolution. At its most basic, $E_T^{miss}$ is defined:
\begin{equation}
E_{x(y)}^{miss}=-\sum_{i\in{\textnormal{hard objects}}}p_{x(y),i} -\sum_{j\in{\textnormal{soft objects}}}p_{x(y),j}
\end{equation}
where overall $E_T^{miss}$ is a vector composed of $x,y$ components. In order to avoid double-counting the same detector signal in multiple particle reconstruction algorithms, hard objects are consider in order: electrons, photons, hadronically decaying $\tau$-leptons, and then jets. Muons have little overlap since they are composed using MS and ID tracks (and muons alone leave tracks in the MS). As particles are reconstructed in this sequence, commonly used signals are rejected to avoid overlap. Another key variable is $\sum E_T$ which is defined
\begin{equation}
\sum E_T = \sum_{\text{electrons}} p_T^e + \sum_{\text{photons}} p_T^\gamma + \sum_{\text{$\tau$-leptons}} p_T^\tau + \sum_{\text{muons}} p_T^\mu + \sum_{\text{jets}} p_T^{jet} - \sum_{\text{unused tracks}} p_T^{track}
\end{equation}
The first five terms show the hard term while the last represents the soft-term. Selections are applied to reconstructed jets and particles to achieve optimal $E_T^{miss}$ for a particular analysis \cite{METPerf}. In the context of $H\rightarrow WW$ we use a "tight" configuration for $E_T^{miss}$ classified by strict conditions on accepted jets. This working point has the greatest pile-up rejection which is integral to our analysis. 

$E_T^{miss}$ reconstruction contains the complexity of each of its component parts and their $p_T$ resolutions all affect total $E_T^{miss}$ composition. Pile-up and total event activity also play a large role in $E_T^{miss}$ performance. Validations for $E_T^{miss}$ are performed on a variety of observables and MC modelling is compared to reconstructed data. Systematic uncertainties are derived from comparing the reproducibility of these observables and the successful modelling of data. Resolution for reconstructed jets and leptons are also propagated to overall $E_T^{miss}$ uncertainty. $E_T^{miss}$ performance is evaluated using $Z\rightarrow \mu\mu$ or $Z\rightarrow e^-e^+$ and $W\rightarrow e\nu$ events, the first with no genuine $E_T^{miss}$ and the second with significant $E_T^{miss}$ from neutrinos. After specific event selection and $E_T^{miss}$ reconstruction, each of these samples $E_T^{miss}$ distributions is studied and data and MC demonstrated to match within uncertainties. Selected performance observables are shown for the full Run-2 sample set in \ref{fig:METPerf}. Here $Z\rightarrow e^+e^-$ events are selected and reconstructed track-based soft term $E_T^{miss}$ and $E_T^{miss}$ significance distributions are shown. Data and MC show good agreement.

\begin{figure}[!h]
    \centering
  \begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{Pictures/ETMissPerf.png}
  \end{minipage}
  \hspace{.5cm}
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Pictures/ETMissSig.png}
  \end{minipage}
    \caption{ a.) Track-based Soft Term $E_T^{miss}$ shown for the complete Run-2 dataset with an integrated luminosity of 140 fb$^-1$. Monte Carlo simulations are compared to data for the tight $E_T^{miss}$ working point. b.) Track-based Soft Term $E_T^{miss}$ significance modelling shown in the tight $E_T^{miss}$ working point  \cite{JETEtmiss}}
    \label{fig:METPerf}
\end{figure}

In this analysis, a number of important $E_T^{miss}$ variables are used. This selection will be further explained in later chapters, but these variables are used to cut on background (particularly from $Z\rightarrow \tau\tau$ events and to select for the signal $H\rightarrow WW\rightarrow \ell\nu\ell\nu$ channel where its two neutrinos require significant $E_T^{miss}$. Some of the variables used for these selections include track-based $E_T^{miss}$, track-based soft term $E_T^{miss}$, $E_T^{miss}$ significance, $m_{\tau\tau}$, and $p_T^{tot}$. The $E_T^{miss}$ soft term is composed of a track-based and calo-based component. The calorimeter-based component is highly dependent on pile-up and so using a track-based soft term reduces overall pile-up dependence. Track-based $E_T^{miss}$ uses only reconstructed ID tracks from the primary vertex and in our analysis this has been just as powerful discriminant for isolating $Z\rightarrow \tau\tau$ background events from VBF signal. $E_T^{miss}$ significance is newly defined and recommended variable from the Jet/$E_T^{miss}$group. Described further in \cite{METSig}, this discriminates real missing energy from momentum resolution effects. Shown in \ref{fig:METPerf},  $E_T^{miss}$ significance peaks at low values if $E_T^{miss}$ likely comes from resolution effects and not from a real invisible particle in the event. Our analysis also defines two variables based on $E_T^{miss}$:$p_T^{tot}$ describes the total transverse momentum from all hard objects in the event and $m_{\tau\tau}$ is defined
\begin{equation}
m_{\tau\tau} = \frac{m_{\ell\ell}}{\sqrt{x_1*x_2}}
\end{equation}
where 
\begin{equation}
\begin{split}
x_1 = \frac{p^{\ell0}_x*p^{\ell1}_y-p^{\ell0}_y*p^{\ell1}_x}{p^{\ell1}_y*E_{Tx}^{miss}-p^{\ell1}_x*E_{Ty}^{miss}+p^{\ell0}_x*p^{\ell1}_y-p^{\ell0}_y*p^{\ell1}_x}, \\
x_2 = \frac{p^{\ell0}_x*p^{\ell1}_y-p^{\ell0}_y*p^{\ell1}_x}{p^{\ell1}_x*E_{Ty}^{miss}-p^{\ell0}_y*E_{Tx}^{miss}+p^{\ell0}_x*p^{\ell1}_y-p^{\ell0}_y*p^{\ell1}_x}
\end{split}
\end{equation}

\section{Electrons}
Accurate reconstruction and calibration of electrons within the ATLAS detector is integral to precision measurements, including this thesis' $H\rightarrow WW$ measurement where an electron is required in the signal's final state. Reconstruction, identification, and energy measurements of electrons and photons are the goals of the Electron and Photon Performance group.  This section will summarize each of these calibration processes and their performances with a focus on electrons as they are particularly important in reconstructing final state particles for $H\rightarrow WW$. 

Electrons are defined through energy deposits in the calorimeter, called superclusters, each with a matching track from the Inner Detector. Photons are defined strictly through a calorimeter cluster though ``converted" photons can be matched to conversion vertexes in the Inner Detecter and these constitute a large fraction of reconstructed photons. Figure \ref{fig:ElectroReco} shows the procedure for electron and photon reconstruction. First, topo-clusters in the EM calorimeter and tracks in the ID are selected and matched together. Topo-clustersis are defined based on signal to noise significance in calorimeter cells and calorimeter cell proximity to one another.Standard reconstruction takes place in the inner detector and potential tracks are asigned to topo-clusters if their positions are within region-of-interest compatible with that topo-clusters EM shower. Next, superclusters are built from track-matched topo-clusters. Topo-clusters are tested for use as seed cluster candidates which begin super-clusters. Then remaining topo-clusters are tested for compatibility as satellite clusters to each seed candidate. The results combination of seed and satellite clusters form super-clusters which are defined independetly for photons and electrons.  Finally, tracks are added to super-clusters, energy calibration and position corrections are applied and analysis-level electrons and photons are created. Reconstruction efficiency for electrons is quite high, particularly at high $p_T$. Photon reconstruction efficiency is significantly lower due to their dependence solely on calorimeter clusters. 

\begin{figure}[!h]
        \centering
    \includegraphics[width=.6\textwidth]{Pictures/ElectroReco.png}
    \caption{ Electron and photon reconstruction algorithm \cite{ElectronPhotonPerformance}}
    \label{fig:ElectroReco}
\end{figure}
 
Electron and photon energy resolution determined from EM calorimeter showers and this is optimized with multivariate regression algorithms. Energy scale is also corrected using calibration from well-known $Z\rightarrow ee$ decays and verified with other $Z$-boson decays. Similarly, these calibrations are calculated for photons using $Z\rightarrow \ell\ell\gamma$. Systematic uncertainties that effect these calibrations include passive material between the interaction point and the EM calorimeter and pile-up. 

While electron and photon objects are already identified and reconstructed, further `identification' selections are used to improve electron and photon purity. Prompt electrons are identified with a likelihood discriminant which takes into account measured inner detector and calorimeter quantities such as track parameters and electromagnetic shower properties. The primary electron track must stretch into the two inner tracking layers closest to the beam line and hit multiple points in the silicon-strip detector. The likelihood discriminant is calculated from probability density functions formed by smoothing histograms of 13 discriminating variables with an adaptive kernel density estimator.  These are calculated separately for both the likelihood that an event reconstructs a prompt electron (signal) or that it does not (background). These likelihoods are derived from well-known $Z\rightarrow ee$ and $J/\Psi\rightarrow ee$ events recorded in Run-2. One electron in each decay must satisfy strict likelihood discriminant requirements from the previously calculated likelihood. The other serves as a probe of the new likelihood dscriminate. Three electron working points are derived- loose, medium, and tight, each with respectively lower efficiencies and higher purity. Physics analyses use the working points optimized for their analysis. For the $H\rightarrow WW$ measurement we use `Medium' reconstructed electrons which has a lower efficiency than the loose condition but a factor 2$\times$ greater rejection of background in electron identification. Figure \ref{fig:ElectronEff} shows electron identification efficiency for each of the working points in a sample of $Z\rightarrow ee$ events where efficiency is calculated through comparisons to MC simulated $Z\rightarrow ee$ and $J/\Psi\rightarrow ee$ events. The average efficiencies for electroweak processes are 93\%, 88\% and 80\% respectively for Loose, Medium, and Tight operating points and gradually increase at high $E_T$

\begin{figure}[!h]
        \centering
    \includegraphics[width=.75\textwidth]{Pictures/ElectroEff.png}
    \caption{ Electron reconstruction efficiency as a function of $E_T$ and $\eta$ in $Z\rightarrow ee$ events for Losse, Medium, and Tight electrons \cite{ElectronPhotonPerformance}}
    \label{fig:ElectroReco}
\end{figure}

Track hits and calorimeter deposits near reconstructed electrons and muons can bias energy, momentum, and position measurements. Isolation performance can be defined for calorimeter clusters by $E_T^{cone}$  the sum of transverse energy within a cone $\Delta R$ near a photon or electron cluster after correcting for leakage and pile-up effects. Track isolation $p_T^{cone}$ is defined as a sum of transverse momentum of tracks within a cone about the electron track or interpolated photon track. For electrons the distance between nearby decay products is directly related to electron $p_T$ so a variable cone size can used such that 
\begin{equation}
\Delta R = \text{min}(\frac{10}{p_T},\Delta R_{max})
\end{equation}
where $\Delta R_{max}$ is typically 0.2. Isolation working points strike a balance between efficiency and rejection of misidentified prompt electrons. The gradient working point is used by the $H\rightarrow WW$ and analysis and gives an efficiency of 90\% at $p_T = 25$GeV and 99\% at $p_T = 60$GeV. These values are reached through cuts on $E_T^{cone20}$ and $p_T^{varcone20}$ derived from $J\Psi\rightarrow ee$ and $Z\rightarrow ee$ MC simulations. Efficiency of different isolation working points for electrons are shown in Figure \ref{fig:ElectronIsoEff}. These are shown in Medium identified electrons for $Z\rightarrow ee$ events and demonstrate the effiencies possible with different cuts on the calorimeter and track-based isolation variables. The Gradient working point delivers reasonably high efficiency that is stable across $\eta$ and is coupled with high background rejection of misidentified electrons.  

\begin{figure}[!h]
        \centering
    \includegraphics[width=.65\textwidth]{Pictures/ElectronIsoEff.png}
    \caption{ Electron isolation effiency shown for four working points in $Z\rightarrow ee$ events as a function of $E_T$ and $\eta$ \cite{ElectronPhotonPerformance}}
    \label{fig:ElectronIsoEff}
\end{figure}

Photon isolation is solely calorimeter-based. $Z\rightarrow \ell\ell\gamma$ events are used for photon isolation efficiency measurements and three working points are available which balance efficiency and background rejection just as in the electron case. 

Electron reconstruction, calibration, identification, and isolation are all integral for the $H\rightarrow WW$ analysis in order to accurately identify electrons in the final state of candidate Higgs events. Systematic uncertainties from electron identification and isolation efficiency will be discussed further in later chapters. 

\section{Muons}
Muons are abundant in the ATLAS detector and help lead to some of the most interesting physics results and analyses produced by the ATLAS experiment, for example, $H \rightarrow 4\ell$ or $Z^\prime \rightarrow \mu^+\mu^-$ searches \cite{4l}. The Muon Combined Performance group is tasked with producing the most accurate muon data for physics analyses. This includes muon reconstruction, identification, isolation, and analysis of efficiency, as well as muon momentum scale and resolution. The group's goal is to create a number of ``working points'' tailored to different types of physics analyses that will isolate, identify, and reconstruct muons in the region of interest to the analyses. The working points are continuously updated and improved before being tested and implemented on different analyses. My work with the MCP group has focused on applying corrections necessary for muon momentum scale at the per mille level and resolution at the percent level in simulation/data. These are derived by a template fit of simulations smeared and corrected by variables in data. Finally, these corrections are validated by comparisons to simulations over a variety of variables. 

Muon reconstruction is performed independently in the ID and MS and the information from these separate sub-detectors is combined to form full tracks. This section will focus on: 1) reconstruction in the ID, which is the same for any charged particle; 2) reconstruction in the MS, which is particular to muons; and 3) the combined reconstruction, which uses information from both the ID and MS. 

\subsection{Muon reconstruction in the ID}
In the ID, a pattern recognition algorithm reconstructs particle tracks with an inside-out sequence \cite{patternrecognition}. A track from a particle traversing the barrel typically has 3 pixel clusters, 8 SCT clusters and more than 30 TRT straw hits. The sequence begins by finding three-dimensional space points from the silicon hits. Each set of three space points which originate in the  interaction point are used to trace hits up to the outer edge of the silicon detector. The final track parameters are fit through a collection of hits that extend to the TRT \cite{IDreconstruction}.

\subsection{Muon reconstruction in the MS}
Muon reconstruction in the MS is not an inside-out procedure like that in the ID. Reconstruction begins with a search for hit patterns in each MS subdetector, which are called segments. The middle of the MS typically exhibits the largest number of trigger hits, therefore tracks are built by working out from the center of the MS and connecting segments layer-by-layer. Criteria such as hit multiplicity and fit quality determine track acceptance. At least two segments are needed to build a track. Hits associated with each track candidate are fitted using a global $\chi^2$ fit. A track candidate is accepted if it passes the selection criteria \cite{ICreconstruction}. 

\subsection{Combined Reconstruction}
The combined ID-MS reconstruction uses different algorithms to find different \textit{muon types}. There are four main types outlined below, but preference-in terms of overlap between types-is given to Combined (CB), then Segment-tagged (ST), and finally Calorimeter Tagged (CT) muons. These algorithms have been continuously been improved to yield better precision, speed, and robustness against misidentification \cite{MCPpaper}.  

\begin{itemize}
\item \textbf{Combined muon (CB)}: This type combines tracks from the ID and MS detectors using a global refit on all hits (some may be removed or added to improve quality). Most muons are reconstructed using an outside-in method. 
\item \textbf{Segment-tagged muon (ST)}: ST muons are assigned an ID track that is associated with at least one local MDT or CSC track after extrapolation. These are used when muons cross only one layer of the MS because of low $p_T$ or regions out of most MS layer boundaries. 
\item \textbf{Calorimeter-tagged muon (CT)}: These muons are identified by an ID track that can be matched to a minimum ionizing particle energy deposit in the calorimeter. These muons have the lowest purity but are optimized for $|\eta|  < 0.1$ and $1.5 < p_T < 100$ GeV where the MS is only partially instrumented. 
\item \textbf{Extrapolated muon (ME)}: These muons are reconstructed in the MS with the addition of silicon points and with a loose requirement that the muon track originated at the IP. In general, this muon is required to traverse $2-3$ layers of MS chambers. These are mainly used to extend acceptance for $2.5 < |\eta| < 2.7$, which is not covered in the ID. 
\end{itemize}

\subsection{Muon Identification}\index{Identification}
In order to identify muons from other particles (like backgrounds from pion and kaon decays) strict quality requirements must be set to select prompt muons with high efficiency. Ideal ``signal'' muons are those that come from $W$ decays (as opposed to light-hadron decays) and originate from the interaction point. We use a few variables to identify muons:
\begin{itemize}
\item \textit{q/p significance}: The absolute value of the difference between the ratio of the charge and momentum of muons in the ID and MS divided by the sum in quadrature of their corresponding uncertainties.
\item \textit{$\rho^\prime$}: The absolute value of the difference between the $p_T$ measurements in the ID and MS divided by the $p_T$ of the combined track. 
\item \textit{$\chi ^2$}: The normalized fit parameter of the combined track.
\end{itemize}

Specific requirements on the number of hits in the ID and MS assure that inefficiencies are expected and momentum measurements are robust. There are four muon identification selections that each addresses specific needs of physics analyses \cite{MCPpaper}.

\begin{itemize}
\item \textbf{\textit{Loose} Muons}: The \textit{Loose} criteria maximizes the reconstruction efficiency, losing very few potential muons, while providing satisfactory tracks. All muon types are used in this criteria, and it is the optimal selection for Higgs boson candidates in the four-lepton final state \cite{4l}.
\item \textbf{\textit{Medium} Muons}: \textit{Medium} is the default selection for muons in ATLAS because it minimizes systematic uncertainties associated with reconstruction and calibration. Only CB and ME tracks are used with requirements for over 3 hits in at least two MDT layers in most regions. All \textit{Medium} muons are included in the \textit{Loose} criteria.
\item \textbf{\textit{Tight} Muons}: \textit{Tight} selects muons with the highest purity, but sacrifices efficiency. All \textit{Tight} muons are included in the \textit{Medium} selection, but only CB muons with at least two hits in the MS are considered, and the $\chi^2$ value must be less than $8$.  
\item \textbf{\textit{High-$p_T$} Muons}: \textit{High-$p_T$} muons have good momentum resolution for tracks with $p_T > 100$ GeV. This is beneficial to searches for high-mass $Z^\prime$ and $W^\prime$ resonances. CB muons in the \textit{Medium} selection with at least $3$ hits in $3$ MS stations are included. 
\end{itemize}

\subsection{Muon Reconstruction Efficiency}\label{sec:efficiency} \index{Efficiency}
We measure the muon reconstruction efficiency in two different ways in the regions $|\eta|  < 2.5$ and $2.5 < |\eta|  < 2.7$. First, in the barrel region, we use the \textbf{Tag-and-Probe} method. In this method we select an almost-pure sample of $J/\psi$ and $Z$ decays. We require the leading muon to be a \textit{Medium} muon labeled the \textbf{tag}. This muon fires the trigger. The subleading muon, the \textbf{probe}, must be reconstructed independently. There are three types of probes:
\begin{itemize}
\item \textbf{ID track}: Allows measurement of MS efficiency and of tracks not accessible to CT muons. 
\item \textbf{CT tracks}: Allows measurement of MS efficiency and has powerful rejection of background (especially at low $p_T$). This is the most commonly used probe. 
\item \textbf{MS tracks}: Allows measurement of ID and CT efficiency.
\end{itemize}
\par \hspace{20pt} To find the overall efficiency of \textit{Medium, Tight,} or \textit{High-$p_T$} muons, we multiply the efficiencies associated with each type of probe. The efficiency  $\epsilon$(X$|$CT) (X $=$ \textit{Medium / Tight / High-$p_T$}) of reconstructing these muons assuming a reconstructed ID track is measured using a CT muon as probe. This result is corrected by the efficiency $\epsilon$(ID$|$MS) of the ID track reconstruction measured using MS probes.
\begin{align*}
	\centering 
    \epsilon \textrm{(X$|$ID)} \cdot \epsilon \textrm{(ID)} = \epsilon \textrm{(X$|$CT)} \cdot \epsilon \textrm{(ID$|$MS)} \hspace{20 pt} (\textrm{X} = Medium/Tight/High\textrm{-}p_T)
\end{align*}
The ID track reconstruction efficiency must be independent from the muon spectrometer track reconstruction ($\epsilon$(ID) $= \epsilon$(ID$|$MS)). In addition, the use of a CT muon as a probe instead of an ID track must not affect the probability for \textit{Medium, Tight,} or \textit{High-$p_T$} reconstruction ($\epsilon$(X$|$ID) $= \epsilon$(X$|$CT)). These assumptions are largely true with simulations showing some small deviations. These deviations are taken into account when calculating systematic errors. 
The reconstruction efficiency of \textit{Loose} muons is measured separately for CT muons within $|\eta| < 0.1$ and all other \textit{Loose} types. The CT muon efficiency is measured using MS probe tracks, and the efficiency of other muons is evaluated similarly to the \textit{Medium, Tight,} and \textit{High-$p_T$} muons using CT probe muons \cite{MCPpaper}.
For $|\eta| > 2.5$, the efficiency is calculated using the ME muons in the \textbf{Loose} and \textbf{Medium} selections. The number of muons observed in this region is normalized to the number of muons observed in the region $2.2 < |\eta| < 2.5$. A more detailed discussion of the efficiency measurement in this region can be found in Ref. \cite{oldMCPpaper}. 
\textbf{Scale factors} are defined as the ratios between the efficiency of data and the efficiency of Monte Carlo simulations. They are used to describe the deviation between simulated and real detector behavior and are used in physics analyses to correct simulations. 
\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{Pictures/efficiencyoverpt.PNG}
  \end{minipage}
  \hspace{.5cm}
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Pictures/efficiencyovereta.PNG}
  \end{minipage}
  \caption{On the left, reconstruction efficiency for \textit{Medium} muons from $Z \rightarrow \mu\mu$ and $J/\psi \rightarrow \mu\mu$ events is displayed as a function of the $p_T$ of the muon in the region $0.1 < |\eta| < 2.5$.  On the right, muon reconstruction efficiency is shown as a function of $\eta$ in $Z \rightarrow \mu\mu$ events for muons with $pT > 10$ GeV for \textit{Medium} and \textit{Loose} muons (squares) in the region $|\eta| < 0.1$, where the \textit{Loose} and \textit{Medium} selections differ significantly. In both plots the error bars on the efficiencies indicate the statistical uncertainty and the bottom panels show the ratio of the measured to predicted efficiencies, with both statistical and systematic uncertainties \cite{MCPpaper}.}
  \label{fig:efficiency}
\end{figure}

Figure \ref{fig:efficiency} displays reconstruction efficiency for \textit{Medium} muons over a range of $p_T$ and $\eta$. $J/\psi$ decays probe low $p_T$ muons while $Z$ decays probe muons of a higher $p_T$ allowing a large range to be defined.  Muon reconstruction works well over a large range of both $p_T$ and $\eta$ as evidenced by an efficiency above $95\%$ throughout ranges shown in $p_T$ and $\eta$. In addition, MC simulations match data quite well - within $1-2\%$. The only significant loss of efficiency is seen with \textit{Medium} muons at extremely low $\eta$ due to criteria excluding ID muons. We can make up this lost efficiency by substituting \textit{Loose} muons for excluded ID muons. Overall, the default \textit{Medium} muon selection demonstrates a high reconstruction efficiency. 

\subsection{Muon Isolation} \index{Isolation}
\par \hspace{20pt} Isolating muons from heavy particles is one of the keys to understanding the background in many physics analyses. When heavy particles like $W$, $Z$, and Higgs bosons decay they often produce muons in isolation. Semileptonic decays, on the other hand, typically produce muons embedded in jets.

\par \hspace{20pt} The MCP group uses two muon isolation variables: a track-based variable ($p_T^{varcone30}$) and a calorimeter-based variable ($E_T^{topocone20}$). $p_T^{varcone30}$ is defined as the scalar sum of the transverse momenta of tracks with $p_T > 1$ GeV in a cone around the muon of transverse momentum $p_T$ excluding the muon track itself. The cone size is $p_T$-dependent to improve the performance for muons produced in decays with a large transverse momentum. $E_T^{topocone20}$ is defined as the sum of the transverse energy of topological clusters in a cone around the muon after subtracting the contribution from the energy deposit of the muon itself and correcting for pile-up effects \cite{jets}. 

\par \hspace{20pt} Table \ref{tab:datasetsdef} defines seven isolation selection criteria - called ``isolation working points'' - that optimize different physics analyses. The\textit{LooseTrackOnly} and \textit{FixedCutTightTrackOnly} working points are defined by cuts on the relative track-based isolation variable. All other working points are defined by cuts applied separately on both relative isolation variables. All cuts are tuned as a function of the $\eta$ and $p_T$ of the muon to obtain a uniform performance. The target efficiencies of the different working points are described in Table \ref{tab:isolation}. The efficiencies for the seven isolation working points are measured in data and simulation using the \textbf{Tag-and-Probe} method described in Section \ref{sec:efficiency} on $Z \rightarrow \mu\mu$ decays. Figure \ref{fig:isolation} shows the isolation efficiency measured for \textit{Medium} muons in data and simulation as a function of the muon $p_T$ for two different working points. In both the \textit{Loose} and \textit{LooseTrackOnly} working points, efficiency is above $98\%$ and matches simulation well within errors. 

\begin{table}[!h]
	\centering 
    \includegraphics[width=.9\textwidth]{Pictures/isolationworkingpoints.PNG}
    \caption{The seven isolation working points are described by their discriminating variables and defining criteria \cite{MCPpaper}.}
    \label{tab:isolation}
\end{table}

\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.47\textwidth}
    \includegraphics[width=\textwidth]{Pictures/isolationefficiency1.png}
  \end{minipage}
  \hspace{.2cm}
  \begin{minipage}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{Pictures/isolationefficiency2.png}
  \end{minipage}
  \caption{Isolation efficiency for the Loose (left) and LooseTrackOnly (right) muon isolation working points. The efficiency is displayed as a function of $p_T$ in $Z \rightarrow \mu\mu$ events. The black markers show efficiency measured in data samples while the red show MC simulations. The bottom panel shows the ratio of the efficiency between the two as well as both  statistical and systematic uncertainties \cite{MCPpaper}.}
  \label{fig:isolation}
\end{figure}

\subsection{Muon Momentum Corrections} \index{Momentum Corrections}
\par \hspace{20pt} The muon momentum scale and resolution are studied using $Z$ and $J/\psi$ decays. In order to obtain agreement between simulation and data in muon momentum scale to the per mille level and in resolution to the percent level, we need to apply a set of corrections to the simulated muon momentum. After applying the corrections we validate them by comparing the muon momentum scale and resolution between simulation and data over $\eta$, $\phi$, and $p_T$.

\par \hspace{20pt} I have been heavily involved in calculating the parameters that govern the muon momentum corrections and validating them with the newest datasets. Within the next section, I will describe muon momentum corrections and the parametrizations that define them.

\par \hspace{20pt} We extract the calibration parameters with the transverse momentum of the ID and MS components of a CB track.  The corrected transverse momentum is described by the following equation: 

\begin{align*}
	\centering 
    p_T^{\textrm{Cor,Det}} = \frac{p_T^{\textrm{MC,Det}}+\sum\limits_{n=0}^1s_n^{\textrm{Det}}(\eta,\phi)(p_T^{\textrm{MC,Det}})^n}{1+\sum\limits_{m=0}^2\Delta r_m^{\textrm{Det}}(\eta,\phi)(p_T^{\textrm{MC,Det}})^{m-1}g_m} .
\end{align*}
\par \hspace{20pt} Here the $g_m$ terms are normally distributed random variables with zero mean and unit width. The $\Delta r $ and $s$ terms describe momentum resolution smearing and scale corrections applied in specific detector regions, respectively. Both the ID and MS are divided into $18$ pseudorapidity regions and the MS is divided into two $\phi$ bins separating the large and small sectors. Each of these bins leverages different alignment techniques and has different material distributions. 

\par \hspace{20pt} There are two $s$ terms that represent different types of corrections. $s_1$ corrects for inaccuracy in the description of the magnetic field integral and the detector in the direction perpendicular to the magnetic field. $s_0$ corrects for the inaccuracy in the simulation of energy loss in the calorimeter and other materials. Since this loss is negligible in the ID, it is only nonzero in the MS \cite{MCPpaper}.

\par \hspace{20pt} The denominator introduces momentum smearing which broadens the $p_T$ resolution in simulation. The parametrization of the smearing is defined:
\begin{align*}
	\centering 
    \frac{\sigma(p_T)}{p_T} = r_0/p_T \oplus r_1 \oplus r_2 \cdot p_T .
\end{align*}
In this equation $r_0$ is related to the fluctuations in energy loss in the traversed material, $r_1$ accounts for multiple scattering, local magnetic field inhomogeneities, and local radial displacements of hits, and $r_2$ describes intrinsic resolution effects caused by the spatial resolution of the hit measurements and by residual misalignment of the MS \cite{MCPpaper}. 

\par \hspace{20pt} Correction parameters are extracted from data using a binned maximum-likelihood fit with templates derived from simulation which compares the invariant mass distributions for $J/\psi$ and $Z$ decay candidates in data and simulation. The muons are carefully selected to be compatible with tracks that start at the interaction point and penetrate both the ID and the MS. Muons are also selected to pass specific momentum and isolation criteria. The dimuon mass distribution of these tracks in data is fitted using a Crystal Ball function convoluted with an exponential background distribution in the ID and MS fits. The background model and its normalization are then used in the template fit. The fits are performed in $\eta-\phi$ regions of fit (ROFs) which compromise regions with uniform features in the ID and MS \cite{MCPpaper}. 

\par \hspace{20pt} From these fits, we can find the smearing terms across all $\eta$ regions. Once the corrections are applied we can validate that the agreement between data and MC is excellent. This is shown in Figure \ref{fig:parametrizationeta}. $r_0$ is set to zero across all $\eta$ regions since energy loss is negligible in the ID. $r_1$ and $r_2$ increase as $\eta$ increases since spatial resolution decreases and inhomogeneities increase as as we move from the barrel to end-cap regions of both the ID and MS. 

\begin{figure}[!h]
	\centering 
    \includegraphics[width=.85\textwidth]{Pictures/parametrizationIDeta.PNG}
    \caption{ The $r$- values from each of $10$ fits of resolution to $p_T$ for ID muon simulations are shown. Each value corresponds to a particular ROF or $\eta$ region. These plots show $r_1$ (left) and $r_2$  (right) as functions of leading muon $\eta$.}
    \label{fig:parametrizationeta}
\end{figure}

\par \hspace{20 pt}
We must continue to study muon momentum corrections during ATLAS runs to validate muon calibration performance and account for discrepancies.
